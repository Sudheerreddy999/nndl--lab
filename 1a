import numpy as np

# Hyperparameters
input_size = 3
hidden_size = 2
output_size = 1
learning_rate = 0.2
epochs = 1000

# Initialize weights and biases (centered, small)
np.random.seed(1)
W1 = np.random.randn(input_size, hidden_size) * 0.1
b1 = np.zeros(hidden_size)
W2 = np.random.randn(hidden_size, output_size) * 0.1
b2 = np.zeros(output_size)

# Training data (one sample)
X = np.array([[1.0, 8.0, 3.0]])   # shape (1, 3)
y = np.array([[0.9]])             # shape (1, 1)

for epoch in range(1, epochs + 1):
    # Forward propagation
    z1 = np.dot(X, W1) + b1               # (1, hidden_size)
    a1 = 1 / (1 + np.exp(-z1))            # sigmoid
    z2 = np.dot(a1, W2) + b2              # (1, 1)
    a2 = 1 / (1 + np.exp(-z2))            # sigmoid

    # Loss (Mean Squared Error as scalar)
    loss = np.mean((y - a2) ** 2)

    # Backward propagation
    dz2 = 2 * (a2 - y) * (a2 * (1 - a2))  # (1, 1)
    dW2 = np.dot(a1.T, dz2)               # (hidden_size, 1)
    db2 = np.sum(dz2, axis=0)             # (1,)

    dz1 = np.dot(dz2, W2.T) * (a1 * (1 - a1))  # (1, hidden_size)
    dW1 = np.dot(X.T, dz1)                     # (input_size, hidden_size)
    db1 = np.sum(dz1, axis=0)                  # (hidden_size,)

    # Update weights and biases
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1

    # Print progress occasionally
    if epoch % 100 == 0 or epoch == 1:
        print(f"Epoch {epoch:4d}  loss={loss:.6e}  pred={a2.flatten()[0]:.6f}")

# Final output
print("\nFinal predicted output:", a2)
print("Final loss:", loss)
Epoch    1  loss=1.454460e-01  pred=0.518626
Epoch  100  loss=1.707992e-03  pred=0.858672
Epoch  200  loss=2.369496e-04  pred=0.884607
Epoch  300  loss=4.859432e-05  pred=0.893029
Epoch  400  loss=1.143435e-05  pred=0.896619
Epoch  500  loss=2.857053e-06  pred=0.898310
Epoch  600  loss=7.345772e-07  pred=0.899143
Epoch  700  loss=1.915580e-07  pred=0.899562
Epoch  800  loss=5.031096e-08  pred=0.899776
Epoch  900  loss=1.326183e-08  pred=0.899885
Epoch 1000  loss=3.502291e-09  pred=0.899941

Final predicted output: [[0.89994082]]
Final loss: 3.502291201837469e-09
