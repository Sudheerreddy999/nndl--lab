import os
import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import recall_score, f1_score, confusion_matrix

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# 1. Load Dataset using kagglehub
# Note: kagglehub.dataset_download should return a local path where files are stored.
path = kagglehub.dataset_download("kartik2112/fraud-detection")
print("Path to dataset files:", path)

# 2. List files to locate CSV
files = os.listdir(path)
print("Files in dataset:", files)

# 3. Load training CSV (fraudTrain.csv)
csv_name = "fraudTrain.csv"
csv_path = os.path.join(path, csv_name)
if not os.path.exists(csv_path):
    raise FileNotFoundError(f"{csv_name} not found in {path}")

df = pd.read_csv(csv_path)
print("Dataset Loaded:", df.shape)

# 4. Drop non-numeric or unwanted columns safely (only drop those that exist)
cols_to_drop = [
    'Unnamed: 0', 'trans_date_trans_time', 'cc_num',
    'merchant', 'category', 'first', 'last',
    'gender', 'street', 'city', 'state',
    'zip', 'lat', 'long', 'city_pop',
    'job', 'dob', 'trans_num'
]
cols_present = [c for c in cols_to_drop if c in df.columns]
df = df.drop(columns=cols_present)
print("After dropping columns:", df.shape)

# 5. Features and target
if "is_fraud" not in df.columns:
    raise KeyError("Target column 'is_fraud' not found in the dataset")
X = df.drop("is_fraud", axis=1)
y = df["is_fraud"].astype(int)  # ensure integer labels

# 6. Train-test split (stratify if possible)
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# 7. Feature scaling
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 8. Neural network (32-16-1)
model = Sequential([
    Dense(32, activation='relu', input_shape=(X_train.shape[1],)),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

model.compile(
    optimizer=Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)
model.summary()

# 9. Train model
history = model.fit(
    X_train, y_train,
    epochs=10,
    batch_size=512,
    validation_split=0.2,
    verbose=1
)

# 10. Loss curve
plt.figure()
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss Curve")
plt.legend()
plt.show()

# 11. Prediction on test set
y_pred_prob = model.predict(X_test, verbose=0).flatten()
y_pred = (y_pred_prob >= 0.5).astype(int)

# 12. Evaluation metrics (handle zero-division safely)
print("Recall Score:", recall_score(y_test, y_pred, zero_division=0))
print("F1 Score:", f1_score(y_test, y_pred, zero_division=0))

# 13. Confusion matrix plot
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5, 4))
sns.heatmap(
    cm,
    annot=True,
    fmt='d',
    cmap='Blues',
    xticklabels=['Normal', 'Fraud'],
    yticklabels=['Normal', 'Fraud']
)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()
